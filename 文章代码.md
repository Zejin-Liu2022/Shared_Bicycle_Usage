Of course, here is all the code from the article.

### **2.2.1 Missing Value Handling**
```python
import pandas as pd

df = pd.read_csv('hour.csv')
print(df.isnull().any())
print(df.isnull().sum())
```

### **2.2.2 Outlier Treatment**
```python
def abnormalIndex(df):
    abnIndex = pd.Series().index
    col = df.columns
    for i in range(len(col)):
        s = df[col[i]]
        a = s.describe()
        high = a['75%'] + (a['75%'] - a['25%']) * 1.5
        low = a['25%'] - (a['75%'] - a['25%']) * 1.5  # Corrected from the article which had a['75%']
        abn = s[(s > high) | (s < low)]
        print(col[i], list(abn.index)) # Corrected from the article to print current column name
        abnIndex = abnIndex.union(abn.index) # Corrected from the article which used '|' operator
    return abnIndex

df = df.drop(abnormalIndex(df))
```

### **2.2.3 Variable Transformation**
```python
cols = ['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit'] # Corrected variable names from the article
for col in cols:
    df[col] = df[col].astype('category')
df.info()
```

### **2.4.2 Correlation Analysis**
```python
import seaborn as sns
import matplotlib.pyplot as plt

corr = df.corr()
plt.figure(figsize=(20, 15))
sns.heatmap(corr, annot=True, annot_kws={'size': 15})
# Viewing the correlation of each feature with 'count'
print(corr['cnt'].sort_values(ascending=False))
```

### **2.5.1 One-Hot Encoding**
```python
pd.get_dummies(df['season'], prefix='season', drop_first=True)
df_oh = df

def one_hot_encoding(data, column):
    data = pd.concat([data, pd.get_dummies(data[column], prefix=column, drop_first=True)], axis=1)
    data = data.drop([column], axis=1)
    return data

cols = ['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit'] # Corrected variable names
for col in cols:
    df_oh = one_hot_encoding(df_oh, col)
print(df_oh.head())
```

### **3.1 Train/Test Split**
```python
from sklearn.model_selection import train_test_split
import sklearn.metrics as sm

X = df_oh.drop(columns=['atemp', 'windspeed', 'casual', 'registered', 'cnt'], axis=1) # Corrected 'count' to 'cnt'
y = df_oh['cnt'] # Corrected 'count' to 'cnt'
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

data = pd.concat([x_train, y_train], axis=1) # Corrected variable name from train_x to x_train
print(data.shape)
```

### **3.2 K-Fold Cross-Validation**
```python
from sklearn import model_selection
from sklearn.ensemble import RandomForestRegressor # Added import for the model used in the loop
models = [RandomForestRegressor()] # Initialize models list

def train(model):
    kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)
    pred = model_selection.cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_squared_error')
    cv_score = pred.mean()
    print('Model:', model)
    print('CV score:', abs(cv_score))

for model in models:
    train(model)
```

### **3.3.2 Random Forest Model**
```python
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)
```

### **3.4 Linear Regression Model**
```python
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler # Added for standardization which is necessary for SGD

scaler = StandardScaler()
x_train_std = scaler.fit_transform(x_train)
# y_train is a Series, needs to be reshaped for scaling
y_train_std = scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()

sgd = SGDRegressor()
sgd.fit(x_train_std, y_train_std)
print(sgd.coef_, sgd.intercept_)
```

### **3.5 Model Evaluation**
```python
import matplotlib.pyplot as plt

error = y_test - y_pred
fig, ax = plt.subplots()
ax.scatter(y_test, error)
ax.axhline(lw=3, color='black')
ax.set_xlabel('Observed')
ax.set_ylabel('Error')
plt.show()
```

### **3.6.1 Min-Max Standardization**
```python
# Note: The article applies this to a dataframe called 'data', I'll assume it's the training data
data = pd.concat([x_train, y_train], axis=1) # Recreating 'data' as in the article
numericalFeatureNames = ['temp', 'hum'] # Corrected column name from 'humidity' to 'hum'
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(data[numericalFeatureNames])
data[numericalFeatureNames] = scaler.transform(data[numericalFeatureNames])
```

### **3.6.2 Model Re-evaluation**
```python
import numpy as np
from sklearn.metrics import mean_squared_error
import sklearn.metrics as sm

# The article creates a new test set, which seems to be the same as the training set
dataTest = pd.concat([x_train, y_train], axis=1) # Recreating dataTest as in the article

train_x = data.copy().drop(["cnt"], axis=1)
train_y = data["cnt"]
test_x = dataTest.copy().drop(["cnt"], axis=1)
test_y = dataTest["cnt"]

model = RandomForestRegressor()
model.fit(train_x, train_y)
pred_y = model.predict(test_x)

print("RandomForestRegressor mean_squared_error:", mean_squared_error(test_y, pred_y)) # Corrected order of arguments
print("RandomForestRegressor explained_variance_score:", sm.explained_variance_score(test_y, pred_y)) # Corrected order of arguments
print(np.sqrt(mean_squared_error(test_y, pred_y)))
```